{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GenTwo-imgcheck.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPmpgk4wGzYPosFkuzMStm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kathmandu777/Img-and-Video-check/blob/main/GenTwo_imgcheck.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AjL1H-bOPmV"
      },
      "source": [
        "# GenTwoの投稿画像検証\r\n",
        "- GenTwoにおいて、投稿された画像が不適切かどうかを判断します。\r\n",
        "\r\n",
        "<br>  \r\n",
        "***\r\n",
        "\r\n",
        "### 設計\r\n",
        "1. 投稿画像の受け取り\r\n",
        "2. 画像認識\r\n",
        "  - YOLO（性能と速度によっては変更）\r\n",
        "  - 現在はSSDを使用\r\n",
        "3. 認識物がNGリストに載っていればリジェクト\r\n",
        "  - 何がリジェクトされたか、ユーザーが分かるようにバウンディングボックスの描画\r\n",
        "4. [ ユーザーが不快画像として報告した画像から、モデルを再構築（転移学習を利用）]\r\n",
        "\r\n",
        "### 検出オブジェクトNGリスト\r\n",
        "- undefined\r\n",
        "\r\n",
        "<br>  \r\n",
        "***\r\n",
        "\r\n",
        "## 知識\r\n",
        "#### 物体検出について\r\n",
        "- 物体検出(object detection)=クラス(カテゴリー)と一緒に物体の位置を予測すること\r\n",
        "- 矩形（バウンディングボックス）= 物体の位置情報を示した図形\r\n",
        "\r\n",
        "\r\n",
        "#### 物体検出のアルゴリズム\r\n",
        "- Faster R-CNN : CNNを利用した物体検出の先駆けであるR-CNNを高速化したモデル。従来のR-CNNは候補領域の抽出・特徴量の計算・物体クラス分類をそれぞれ別個の処理として行っていたため高速な物体検出の処理が困難であった。このモデルではこれらの処理を1つの畳み込みCNNにまとめることで高速な物体検出の処理を実現している。\r\n",
        "\r\n",
        "- Mask R-CNN : Faster R-CNNの改良モデル。通常の物体検出のように物体の位置を矩形領域ではなく物体の輪郭に沿った形で推定するセグメンテーションも同時に行うマルチタスク学習に分類される。輪郭に沿った形で物体を検出できるため人間の手や足の関節の位置を推定して骨格に相当する情報を抽出する姿勢推定への応用が可能。\r\n",
        "\r\n",
        "- YOLO : You Only Look Onceの略で「見るのは一度だけ」という意味。従来の手法では候補領域の抽出の際に入力画像をスキャンしていたため入力画像を何度も見る必要があった。YOLOは入力画像をグリッドに分割してからそれぞれのグリッドに物体がふくまれている確率を推定する回帰問題として物体検出を定式化することにより、入力画像を1回だけ見れば物体検出ができるようになっている。これにより、Faster R-CNNと同様に1つの畳み込みCNNでモデルを構成することができるようになり高速な処理が可能となった。\r\n",
        "\r\n",
        "- SSD : Single Shot MultiBox Detector の略。2016年に提案されたモデル。画像に8732個のデフォルトボックスを敷き詰め、デフォルトボックスごとに位置推定とクラス分類を実行する。ボックスと物体の位置の差分を計算する方法により、ネットワーク処理がシンプルになり大幅に速度が向上。リアルタイムでの物体検出を実現している。\r\n",
        "\r\n",
        "\r\n",
        "### 参考文献\r\n",
        "- [markdown記法](https://qiita.com/tbpgr/items/989c6badefff69377da7#%E5%BC%95%E7%94%A8)\r\n",
        "- [TensorHubについて](https://kamujun.hatenablog.com/entry/2018/08/10/183201)\r\n",
        "- [colabでのTensorFlow Hub使用方法](https://qiita.com/code0327/items/3b23fd5002b373dc8ae8)\r\n",
        "- [TensorHubの物体検出学習済みモデル](https://tfhub.dev/s?module-type=image-object-detection)\r\n",
        "- [物体検出について](https://qiita.com/mshinoda88/items/9770ee671ea27f2c81a9)\r\n",
        "- [機械学習における正解率の各指標まとめ](https://qiita.com/cv_carnavi/items/08e11426e2fac8433fed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQrJfZ2pNrml"
      },
      "source": [
        "# 環境変数を追加\r\n",
        "# 検出器のモジュールを格納するフォルダの設定\r\n",
        "import os\r\n",
        "os.environ['TFHUB_CACHE_DIR'] ='/content/tfhub'\r\n",
        "\r\n",
        "# 環境変数の確認\r\n",
        "!printenv TFHUB_CACHE_DIR\r\n",
        "\r\n",
        "# TensorFlow2.xに切り替え\r\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPVOquJ0jnt7"
      },
      "source": [
        "# 検出器のロード\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "\r\n",
        "print(\"loading model\")\r\n",
        "\r\n",
        "# tensorHubにある好きなモデルを指定\r\n",
        "\"\"\"\r\n",
        "wildspeed.jpgで実験\r\n",
        "\r\n",
        "https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1 = 45.637sec 精度はまあ良い\r\n",
        "https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1 = 0.262sec 余分なところも反応している\r\n",
        "https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1 = 71.238sec 精度は素晴らしい\r\n",
        "https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1 = 6.89sec\r\n",
        "\"\"\"\r\n",
        "module_handle = \"https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1\"\r\n",
        "\r\n",
        "# tensorflow製かどうか\r\n",
        "isTensorFlow = \"tensorflow\" in module_handle\r\n",
        "\r\n",
        "#tensorflow製の時\r\n",
        "if(isTensorFlow):\r\n",
        "  detector = hub.load(module_handle)\r\n",
        "else:\r\n",
        "  detector = hub.load(module_handle).signatures['default']\r\n",
        "\r\n",
        "print(\"load end\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IC58klWP1pk"
      },
      "source": [
        "# COCO2017のclass_id <==> class_name の変換辞書作成\r\n",
        "if(isTensorFlow):\r\n",
        "  !wget https://raw.githubusercontent.com/nightrome/cocostuff/master/labels.txt\r\n",
        "\r\n",
        "  id_to_name_dic={}\r\n",
        "  with open(\"labels.txt\",mode=\"r\") as f:\r\n",
        "    for line in f:\r\n",
        "      i,name=line.split(\":\")\r\n",
        "      id_to_name_dic[i]=str(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlNjvPEYkLVJ"
      },
      "source": [
        "# 検出結果を画像にオーバーレイ表示する関数定義\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.patheffects as pe \r\n",
        "\r\n",
        "def showImage(img, r, min_score=0.1):\r\n",
        "  fig = plt.figure(dpi=150,figsize=(4,4))\r\n",
        "  ax = plt.gca()\r\n",
        "  ax.tick_params(axis='both', which='both', left=False, \r\n",
        "                 labelleft=False, bottom=False, labelbottom=False)\r\n",
        "  ax.imshow(img)\r\n",
        "\r\n",
        "  decode = np.frompyfunc( lambda p : p.decode(\"ascii\"), 1, 1)\r\n",
        "\r\n",
        "  boxes =       r['detection_boxes']\r\n",
        "  scores =      r['detection_scores']\r\n",
        "  class_names = r['detection_classes'] if isTensorFlow else r['detection_class_entities']\r\n",
        "\r\n",
        "  if isTensorFlow:\r\n",
        "    boxes=boxes[0]\r\n",
        "    class_names=class_names[0]\r\n",
        "    scores=scores[0]\r\n",
        "\r\n",
        "  n = np.count_nonzero(scores >= min_score)\r\n",
        "\r\n",
        "  # class_names に対応した 色の準備\r\n",
        "  class_set = np.unique(class_names[:n])\r\n",
        "  colors = dict()\r\n",
        "  cmap = plt.get_cmap('tab10')\r\n",
        "  for i, v in enumerate(class_set):\r\n",
        "    colors[v] =cmap(i)\r\n",
        "\r\n",
        "  # 矩形を描画 スコアが低いものから描画\r\n",
        "  img_w = img.shape[1]\r\n",
        "  img_h = img.shape[0]\r\n",
        "  for i in reversed(range(n)):\r\n",
        "    if(isTensorFlow):\r\n",
        "      class_name=id_to_name_dic[str(int(class_names[i]))]\r\n",
        "      score=float(scores[i])\r\n",
        "    else:\r\n",
        "      class_name=class_names[i]\r\n",
        "      score=scores[i]\r\n",
        "\r\n",
        "    text = f'{class_name} {100*score:.0f}%'\r\n",
        "    color = colors[class_names[i]]\r\n",
        "    y1, x1, y2, x2 = tuple(boxes[i])\r\n",
        "    y1, y2 = y1*img_h, y2*img_h\r\n",
        "    x1, x2 = x1*img_w, x2*img_w\r\n",
        "\r\n",
        "    # 枠\r\n",
        "    r = plt.Rectangle(xy=(x1, y1), width=(x2-x1), height=(y2-y1),\r\n",
        "                      fill=False, edgecolor=color, joinstyle='round', \r\n",
        "                      clip_on=False, zorder=8+(n-i) )\r\n",
        "    ax.add_patch( r )\r\n",
        "\r\n",
        "    # タグ：テキスト\r\n",
        "    t = ax.text(x1+img_w/200, y1-img_h/300, text, va='bottom', fontsize=6, color=color,zorder=8+(n-i))\r\n",
        "    t.set_path_effects([pe.Stroke(linewidth=1.5,foreground='white'), pe.Normal()])\r\n",
        "    fig.canvas.draw()\r\n",
        "    r = fig.canvas.get_renderer()\r\n",
        "    coords = ax.transData.inverted().transform(t.get_window_extent(renderer=r))\r\n",
        "    tag_w = abs(coords[0,0]-coords[1,0])+img_w/100\r\n",
        "    tag_h = abs(coords[0,1]-coords[1,1])+img_h/120\r\n",
        "\r\n",
        "    # タグ：背景\r\n",
        "    r = plt.Rectangle(xy=(x1, y1-tag_h), width=tag_w, height=tag_h,\r\n",
        "                      edgecolor=color, facecolor=color,\r\n",
        "                      joinstyle='round', clip_on=False, zorder=8+(n-i))\r\n",
        "    ax.add_patch(r)\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypxzVNv-kEgl"
      },
      "source": [
        "# 物体検出を実行する関数定義\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import PIL.Image as Image\r\n",
        "\r\n",
        "def run_detector(detector, path):\r\n",
        "\r\n",
        "  # 画像を読み込んで detector に入力できる形式に変換\r\n",
        "  img = Image.open(path) # Pillow(PIL)\r\n",
        "  if img.mode == 'RGBA' :\r\n",
        "    img = img.convert('RGB')\r\n",
        "  \r\n",
        "  converted_img = img.copy()\r\n",
        "  converted_img = converted_img.resize((227,227),Image.LANCZOS) # 入力サイズに縮小\r\n",
        "  converted_img = np.array(converted_img, dtype=np.float32)     # np.arrayに変換\r\n",
        "  if(not isTensorFlow):\r\n",
        "    converted_img = converted_img / 255. # 0.0 ～ 1.0 に正規化\r\n",
        "  converted_img = converted_img.reshape([1,227,227,3])\r\n",
        "  converted_img = tf.constant(converted_img)\r\n",
        "  \r\n",
        "  if(isTensorFlow):\r\n",
        "    converted_img = tf.cast(converted_img, dtype=tf.uint8)\r\n",
        "\r\n",
        "  t1 = time.time()\r\n",
        "  result = detector(converted_img) # 一般物体検出（本体）\r\n",
        "  t2 = time.time()\r\n",
        "  print()\r\n",
        "  print(f'検出時間 : {t2-t1:.3f} 秒' )\r\n",
        "  # 結果をテキスト出力するための準備\r\n",
        "  r = {key:value.numpy() for key,value in result.items()}\r\n",
        "  boxes =       r['detection_boxes']\r\n",
        "  scores =      r['detection_scores']\r\n",
        "  decode = np.frompyfunc( lambda p : p.decode('ascii'), 1, 1)\r\n",
        "  class_names = r['detection_classes'] if isTensorFlow else r['detection_class_entities']\r\n",
        "\r\n",
        "  if isTensorFlow:\r\n",
        "    boxes=boxes[0]\r\n",
        "    class_names=class_names[0]\r\n",
        "    scores=scores[0]\r\n",
        "\r\n",
        "  # スコアが 0.25 以上の結果（n件）についてテキスト出力  閾値はデプロイ時に最適化する\r\n",
        "  print(f'検出オブジェクト' )\r\n",
        "  n = np.count_nonzero(scores >= 0.25 )\r\n",
        "  for i in range(n):\r\n",
        "    y1, x1, y2, x2 = tuple(boxes[i])\r\n",
        "    x1, x2 = int(x1*img.width), int(x2*img.width)\r\n",
        "    y1, y2 = int(y1*img.height),int(y2*img.height)\r\n",
        "    if(isTensorFlow):\r\n",
        "      class_name=id_to_name_dic[str(int(class_names[i]))]\r\n",
        "      score=float(scores[i])\r\n",
        "    else:\r\n",
        "      class_name=class_names[i]\r\n",
        "      score=scores[i]\r\n",
        "\r\n",
        "    t = f'{class_name:10} {100*score:3.0f}%  '\r\n",
        "    t += f'({x1:>4},{y1:>4}) - ({x2:>4},{y2:>4})'\r\n",
        "    print(t)\r\n",
        "\r\n",
        "  showImage(np.array(img), r, min_score=0.25) # 検出結果を画像にオーバーレイ(上で定義した関数)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8yvU6kikhtW"
      },
      "source": [
        "# 物体検出\r\n",
        "from google.colab import files\r\n",
        "\r\n",
        "print(\"\\n物体検出したい画像を選択してください(複数選択可)\")\r\n",
        "uploaded = files.upload() # 分類したい画像を選択\r\n",
        "  \r\n",
        "for fn in uploaded.keys():\r\n",
        "  run_detector(detector, fn)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}